#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2019/6/18 23:16
# @Author  : Micky
# @Site    :
# @File    : 01_AdaBoost算法的过程理解代码.py
# @Software: PyCharm

import numpy as np

"""
计算信息熵
"""
def entropy(p):
    return np.sum([-t * np.log2(t) for t in p])

if __name__ == '__main__':
    # 计算所有数据的信息熵
    h = entropy([0.6,0.4])
    # 可知ppt上有三个划分节点2.5，5.5,8.5,分布计算其信息增益
    # 条件熵 2.5划分
    h1 = 0.3 * entropy([1]) + 0.7 * entropy([0.4/0.7,0.3/0.7])
    g1 = h - h1
    print(g1)
    # 5.5划分
    h2 = 0.6 * entropy([0.3/0.6,0.3/0.6]) + 0.4 * entropy([0.3/0.4,0.1/0.4])
    g2 = h - h2
    print(g2)
    # 8.5 划分
    h3 = 0.1 * entropy([1]) + 0.9 * entropy([0.6/0.9,0.3/0.9])
    g3 = h - h3
    print(g3)
    # 可知2.5划分最优
    # 如果以2.5划分的话，右子树有三个错误的预测，而每个预测的概率为0.1
    err1 = 0.3
    alpha = 0.5 * np.log((1-err1)/err1)
    print("第一个模型的权重系数：{}".format(alpha))
    # 正常样本权重变化（预测7个正常样本）
    w1 = 0.1 * np.e ** (-alpha)
    # 异常样本权重变化(预测失败3个样本)
    w2 = 0.1 * np.e ** alpha
    print(w1,w2)
    # 归一化权重
    z1 = w1 * 7 + w2 * 3
    print('z1：{}'.format(z1))
    w1 = w1/z1
    w2 = w2/z1
    print('归一化后的权重，w1：{},w2：{}'.format(w1,w2))
    print('=================================================')
    # 更新权重后计算所有数据的信息熵
    h = entropy([0.0714 * 7 , 0.1667 * 3])
    print('第二轮所有数据的信息熵：{}'.format(h2))
    # 2.5划分
    h21 = 0.0714 * 3 * entropy([1]) + (0.0714 * 4 + 0.1667 * 3) * entropy([(0.0714/(0.0714 * 4 + 0.1667 * 3)) * 4,(0.1667/(0.0714 * 4 + 0.1667 * 3)) * 3])
    print('第二轮以2.5做为分割点的条件熵：{}'.format(h2))
    g21 = h - h21
    print('第二轮以2.5做为分割点的信息增益：{}'.format(g21))
    # 5.5划分
    h22 = (0.0714 * 6) * entropy([0.5,0.5])  + (0.1667 * 3 + 0.0714 * 1) * entropy([0.0714 /(0.1667 * 3 + 0.0714 * 1),(0.1667/(0.1667 * 3 + 0.0714 * 1)) * 3])
    print('第二轮以5.5做为分割点的条件熵：{}'.format(h22))
    g22 = h - h22
    print('第二轮以5.5做为分割点的信息增益：{}'.format(g22))
    # 8.5划分
    h23 = 0.0714 * entropy([1]) + (0.0714 * 6 + 0.1667 * 3) * entropy([(0.0714 /(0.0714 * 6 + 0.1667 * 3)  * 3) + 0.1667/(0.0714 * 6 + 0.1667 * 3) * 3,0.0714 /(0.0714 * 6 + 0.1667 * 3) * 3])
    print('第二轮以8.5做为分割点的条件熵：{}'.format(h23))
    g23 = h - h23
    print('第二轮以8.5做为分割点的信息增益：{}'.format(g23))
    # 以8.5划分为最优 x < 8.5 1, x>8.5-1
    err = 0.0714 * 3
    alpha2 = 0.5 * np.log((1-err)/err)
    print('第二个模型的权重系数：{}'.format(alpha2))
    # 计算各样本权重(正确的预测数量为7)
    # 上一次预测正确，这一次也正确
    w21 = w1 * np.e**(-alpha2)
    # 上一次预测失败，这一次预测正确
    w22 = w2 * np.e**( - alpha2)
    # 上一轮预测正确，这一轮预测失败
    w23 = w1 * np.e ** (alpha2)
    z2 = w21 * 4 + w22 * 3 + w23 * 3
    w21 = w21 / z2
    w22 = w22/z2
    w23 = w23/z2
    print('第二轮进过归一化后更样本的权重系数 W21：{},W22:{},w23:{}'.format(w21,w22,w23))


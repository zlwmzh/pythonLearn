#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2019/6/11 14:04
# @Author  : Micky
# @Site    : 
# @File    : 01_决策树构建过程代码理解.py
# @Software: PyCharm


"""
决策树划分过程
1. 将所有特征看成一个个节点
2. 遍历当前特征的每一种划分方式，找到最好的分割方式，将数据划分为不同的子节点。 选择信息熵最小的

选择让数据集数据更纯的方式特征属性进行划分。 分类算法：也就是以信息熵进行纯度衡量。选择信息熵最小的方式为最优划分
由于划分后存在多个数据子集，实际计算的是条件熵

NOTE：如果把信息熵或GINI系数做为纯度的衡量标准当成损失函数来看，也会发现决策树的构建也是在往损失函数最小的方向构建
决策树算法是一个贪心算法，是一个选择对于当前数据集最后的划分，只考虑局部特征信息的影响
NOTE：可以认为信息增益越大的特征属性划分，对数据的影响越重，基于这个划分可以更容易将数据划分开
决策树算法将决策能力强的特征属性放到根节点进行划分
"""

import numpy as np

"""
 信息熵：越高，样本概率越平均，没有突出特征，系统越混乱  一个体系/数据集中所有的事件/类别出现的可能性是比较均等的，没有突出的
         越低，样本概率有突出的，系统比较有序。一个体系/数据集中存在某个事件/类别出现的可能性远高于其他事件/类别，也就是有一个非常突出的存在
 条件熵：多个数据子集信息熵的加权均值，权重系数为数据子集的数据量占比
 p：概率集合
"""
def entropy(p):
    return np.sum([- t * np.log2(t) for t in p if t != 0])

"""
gini系数：衡量纯度的标准 越小越纯
"""
def gini(p):
    return 1 - np.sum([t ** 2 for t in p])

"""
决策树中对于纯度的衡量
"""
def h(p):
    return entropy(p)

# 此代码以ppt上的房地产为例
if __name__ == '__main__':
    # 第一个判断节点的选择
    # 1.0 计算整个数据集的纯度  可知否：7 占比0.7 是：3  占比0.3
    h0 = h([0.7, 0.3])
    print("整个数据集的信息熵为：{}".format(h0))
    # 1.1 计算以房地产作为分割点的条件熵
    # 第一个分支，房地厂全都是‘是’, 4个样本都是否
    p11 = 0.4
    h11 = h([1])
    print('以房地产作为分割点，第一个分支的信息熵：{}'.format(h11))
    # 第二个分支，房地厂都是'否',6个样本，3个是，3个否
    p12 = 0.6
    h12 = h([0.5,0.5])
    print('以房地产作为分割点，第二个分支的信息熵：{}'.format(h12))
    # 计算以房地产为分割点的条件熵
    h1 = p11 * h11 + p12 * h12
    print('以房地产作为分割点，条件熵：{}'.format(h1))
    # 计算信息增益
    g1 = h0 - h1
    print('以房地产作为分割点，信息增益：{}'.format(g1))

    # 1.2 计算以婚姻情况作为分割点的时候条件熵(遍历数据计算x2为单身、离婚、已婚的概率)
    # 第一个分支: 单身；样本数目：4，2个是，2个否
    p21 = 0.4
    h21 = h([0.5, 0.5])
    # 第二个分支：离婚；样本数目：3，1个是，2个否
    p22 = 0.3
    h22 = h([1.0 / 3, 2.0 / 3])
    # 第三个分支: 已婚，样本数目：3，3个否
    p23 = 0.3
    h23 = h([1.0])
    # 条件熵
    h2 = p21 * h21 + p22 * h22 + p23 * h23
    # 信息增益
    g2 = h0 - h2
    print("以婚姻作为划分特征的时候，信息增益为:{}".format(g2))

    # 1.3. 以年收入80作为划分值
    # 第一个分支: 小于等于80；样本数目：2，2个否
    p31 = 0.2
    h31 = h([1.0])
    # 第二个分支：大于80；样本数目：8，3个是，5个否
    p32 = 0.8
    h32 = h([3.0 / 8, 5.0 / 8])
    h3 = p31 * h31 + p32 * h32
    g3 = h0 - h3
    print("以年收入80作为划分特征的时候，信息增益为:{}".format(g3))

    # 1.4. 以年收入97.5作为划分值
    # 第一个分支: 小于等于97.5；样本数目：5，3个是，2个否
    p41 = 0.5
    h41 = h([2.0 / 5, 3.0 / 5])
    # 第二个分支：大于97.5；样本数目：5，5个否
    p42 = 0.5
    h42 = h([1.0])
    h4 = p41 * h41 + p42 * h42
    g4 = h0 - h4
    print("以年收入97.5作为划分特征的时候，信息增益为:{}".format(g4))

    print("=" * 100)
    # 特征属性这里不允许重复选择
    # 二、针对于左子树继续划分(年收入小于等于97.5的数据；样本数目：5，3个是，2个否)
    h0 = h41

    # 2.1 计算以房产作为分割点的时候条件熵
    # 第一个分支: 是；样本数目：1，全部为否
    p11 = 0.2
    h11 = h([1.0])
    # 第二个分支：否；样本数目：4，3个是，1个否
    p12 = 0.8
    h12 = h([0.75, 0.25])
    # 条件熵
    h1 = p11 * h11 + p12 * h12
    g1 = h0 - h1
    print("以房产作为划分特征的时候，信息增益为:{}".format(g1))
    # 2.2 计算以婚姻情况作为分割点的时候条件熵(遍历数据计算x2为单身、离婚、已婚的概率)
    # 第一个分支: 单身；样本数目：2，2个是
    p21 = 0.4
    h21 = h([1.0])
    # 第二个分支：离婚；样本数目：2，1个是，1个否
    p22 = 0.4
    h22 = h([0.5, 0.5])
    # 第三个分支: 已婚，样本数目：1，1个否
    p23 = 0.2
    h23 = h([1.0])
    h2 = p21 * h21 + p22 * h22 + p23 * h23
    g2 = h0 - h2
    print("以婚姻作为划分特征的时候，信息增益为:{}".format(g2))
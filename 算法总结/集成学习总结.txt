一. 什么是集成学习(Ensemble Learning)？
    思想：将若干学习器(分类器/回归器)合并到一起产生一个新的学习器。
    简单直白理解：就是将多个子模型合并到一起进行模型的预测。常见的有Bagging、Boosting、Stacking。

Note：弱分类器：准确率稍微好于0.5的分类器。

二. 为什么需要Ensemble Learning?
    1. 弱分类器存在一定差异性，这会导致分类的边界不同，也就是说可能存在错误。将多个弱分类器合并后，也就能够得到一个合理的边界，减少整体的
    错误率，实现更好的效果。
    2. 对于数据集过大或过小，可以进行划分和又放回的操作产生不同的数据子集，使用不同的数据子集产生不同的分类器，最终合成一个大的分类器
    3. 如果数据的划分边界过于复杂，使用线性模型很难描述情况，可以训练多个模型，然后进行模型的融合
    4. 对于多个异构的特征集的时候，很难进行融合，那么可以考虑每个数据集构成一个分类模型，然后多个模型融合。（如手机和笔记本）

因为单个模型可能存在偏向，也就导致每个模型有其自由的优点、缺点。“尺有所短寸有所长”
有放回的抽样：
  从数据集中随机抽取一个样本，记录样本信息后，将样本放回到数据集中，然后重复这个抽取操作，这个过程叫做又放回的抽样。
不放回抽样：
  从数据集随机抽取一个样本，记录信息后，从剩下的样本中继续抽取，这个叫做不放回的数据抽样。

三. Bagging
    定义：在具有N个数据样本的原始数据集上，使用有放回的随机抽样抽取N条数据组成的新的样本数据子集，然后用这个数据子集训练子模型。采用这种方式
    训练s个子模型，将这s个子模型合并就是一个Bagging模型。
    目的：解决模型过拟合的问题。
    期望通过多模型的融合，让模型的泛化能力更强，模型更稳定
    Bagging训练的模型在预测分类/回归的时候，使用多数表决或者求平均值的方式来处理。
    Bagging模型底层的子模型只能是同一种类型。

Note：可能存在某些噪声数据仅在部分数据子集中存在，而其他数据子集不存在，基于这样的数据子集训练出来的模型，也就将异常样本造成的影响降低到最低。
Note：因为在数据产生时用的是有放回的数据抽样，所以对于每个数据子集/子模型是存在部分数据没有参与到训练模型中来的，我们将这部分数据称为袋外数据(out of bag)；
当前子模型训练好后，将对应的袋外数据进行评估(score API)，就可以得到当前模型的袋外评估，将所有评估值均值即可得到oob_score;
Note: Bagging方式是有放回的抽样，并且每个数据子集的样本数量必须与原始样本数量一致，模型训练的时候允许存在重复的数据。
Note：差不多存在1/3样本数据是不在Bagging模型的每个子模型的训练数据中的。

训练过程：
    a. 划分得到原始的训练集(m个样本)
    b. 通过有放回的方式获取s个具有m个样本的数据子集
    c. 分布用b中获取的s个样本训练得到s个子模型
    d. 将s个子模型合并得到最终的模型

预测过程：
    a. 带预测样本
    b. 分布用s个子模型对样本进行预测，得到s个预测结果
    c. 如果是分类问题，就通过多数表决计算最终样本分类，如果是回归问题，就通过均值的方式计算样本的目标值。

四. 随机森林(Random Forest)
    在Bagging的基础上进行修改的一种算法。底层的子模型采用的是决策树。
    RF算法过程：
    1. 从原始样本集(m个样本)采用Bootstrap(有放回的抽样)选出m个样本组成的数据集
    2. 使用m个样本组成数据子集训练决策树；从所有的特征属性中随机选取k个特征属性，然后在这k个特征属性中选择最优的划分方式当作当前的
    划分属性，按照这种方式进行迭代生成决策树
    3. 重复上述两步s次，生成s棵决策树
    4. 这s棵决策树形成RF，通过投票表决决定数据分类。

    Note：
    RF算法中存在两个随机：
    1. 对于每个子模型/决策树，用于训练子模型的训练数据是从原始数据中有放回的随机抽样产生的
    2. 在每个决策树构建过程中，对于每个分割点的分割属性的选择都是从所有的特征属性中随机选择k个特征属性，然后再从这k个特征属性中选择最优的作为
    当前节点的分割属性。
    从样本和特征属性两个方面解决过拟合问题。


五. Bagging和RF的联系和区别
    1. 联系：
       RF属于Bagging体系，主要在Bagging上做了一个修改，目的也是缓解过拟合
    2. 区别：
       a. Bagging底层的子模型可以是任意模型(eg:KNN,决策树...)，而RF的底层子模型必须是决策树
       b. RF构建决策树的过程中，特征属性存在随机选择的过程

六. 决策树
    决策树构建的时候，会选择让数据划分最‘纯’的方向进行划分，也就是训练数据上区分能力最强的特征属性以及划分节点进行数据划分(训练数据上最优的划分方式)。在深度
    比较深的时候都是非常契合训练数据。也就可能存在决策树中某个决策路径是契合训练数据，但是对于测试数据是不太友好的。
    Note：在决策树中，一个叶子节点中所有的样本应该是满足相同的数据特征信息的，所以可以认为如果多个样本落在同一个叶子节点中，可以认为这些样本属于同一个类别。

七. Extra Tree: 极端随机森林
    是RF的一个变化，原理基本和RF一样。主要区别：
    1. RF的子模型训练数据都是有放回抽样产生的，也就是数据子集其实是不一样的，而Extra Tree每个子模型的训练数据是完全一样的，直接采用原始数据进行训练
    2. RF生成决策树时会选择最优的划分属性进行分割，而Extra Tree则是每次直接从特征属性中随机一个特征，并且随机选择一个分割点（value）进行数据划分。
    Note：一般情况下Extra Tree 决策树规模大于RF。方差相对于RF进一步减少

八. Totally Random Trees Embedding(TRTE)
    利用决策树中，多个样本落在相同的叶子节点的话，可以认为属于同一个类别的，那么构建多颗决策树，然后将样本落在叶子节点的信息转化为特征矩阵的形式，从而达到一个
    维度扩展的效果。(类似多项式扩展)
    和RF的区别：
    1. 随机森林的子模型训练数据是有放回抽样产生，那也就是每个子数据集是不一样的，TRTE每个子模型的训练数据是完全一样的
    2. RF是有监督学习模型，TRTE是无监督的学习模型
    3. TRTE底层实际上是回归树(每个子模型的Y值是完全随机的)

九. Isolation Forest(IForest)
	IForest是一种异常点检测算法，使用类似RF的方式检测异常点;
	和RF的区别：
	1. RF的子模型的训练数据都是有放回的随机抽样产生的，每个子模型的训练数据集都是不同的。IForest的每个子模型的训练数据都是基于原始数据采取不放回的抽样
	产生的部分数据进行模型训练（正常样本数据，默认最少256条数据参与模型训练，不满256条的话全部参与训练）
	2. IForest 和TRTF 是完全一样的，每个子模型的Y值是完全随机的，因为子模型仅仅是为了学习数据特征
	3. IForest认为越靠近根节点的叶子节点越可能是异常样本点
	Note：学习正常样本的所有特征，所有不满足正常样本特征的都属于异常样本

十. RF总结：
  ·1. 主要优点:
      a. 训练可以并行化，对于大规模训练具有速度优势
	  b. 优化随机选择特征属性进行分割划分，所以对于高纬度数据，仍然具有比较高的训练性能。
	  c. 可以给出各个特征属性的重要性列表
	  e. 由于存在随机抽样，泛化能力强，方差小，能够缓解过拟合情况
	  f. 实现简单
	  g: 对于缺失的特征属性不敏感
	2. 主要缺点：
      a. 在某些噪音比较大的特征上（数据特别异常情况），RF模型容易陷入过拟合；
      b. 取值比较多的划分特征对RF的决策会产生更大的影响，从而有可能影响模型的效果。

十一. Boosting
    为了解决欠拟合问题，让模型在训练数据上拟合更好。
    思想：通过迭代产生不同的子模型，让所有子模型的融合模型在训练数据上具有一个比较好的效果，每个子模型都是在它之前模型的效果上进行训练改进的。
    可以用于回归和分类问题。底层子模型可以是任意类型。
    Boosting模型的创建过程：
    1. 由原始训练集(m个样本)训练得到第一个子模型
    2. 评估第一个子模型的效果，对于预测错误的样本增加其权重，预测正确的样本减小权重，重新调整训练集数据(m个样本)，重新训练得到子模型2
    3. 重复2步骤，得到s个子模型，将这s个子模型融合得到最终的模型，预测结果加权融合。

十二. AdaBoost算法
    AdaBoost是一种迭代算法，每轮迭代都会在训练集上产生一个子模型，然后用这个子模型对训练数据进行预测，并评估样本的重要性，然后调整样本的权重系数，用新
    调整的训练集训练得出新的子模型。也就是后一个子模型是在前一个的基础上得到的。如果某个样本点被预测的越正确，它的权重减低，否则提高样本权重。权重越高的样本在
    下一次迭代中的权重越大，也就是越难区分的样本在训练过程中将会变的越来越重要。
    AdaBoost算法构建过程：
    1. 假设训练集数据T = {(X1,Y1),(X2,Y2)...(X3,Y3)}
    2. 初始化训练数据权重分布
        D1 = (W11,W12,...W1i...,W1n),W1i = 1/n, i = 1,2,...,n
    3. 使用具有权重值D1的训练数据集学习，得到第一个基本分类器
          G1(x):x ->{-1,+1}

    4. 计算G1(x)在训练数据集上的分类误差
    5. 计算G1(x)的权重系数α1:
        α1 = 1/2 * ln((1-误差)/误差)
    6.  计算第二个子模型的训练数据的权值分布(用到归一化)：
        D2 = (W21,W22,...W2i,W2n)
    7. 依照上述步骤迭代m个子模型，构建基本分类器的线性组合:
        f(x) = sum(αmGm(x))
    8. 得到最终分类器：
        G(x) = sign(f(x))

    样本权重作用：在模型构建的时候，如果样本权重不一致，那么模型构建的时候会尽量让权重大的样本预测正确。
    AdaBoost算法中包含两个权重：
    1. 样本权重：越难区分的样本具有更高的权重，样本损失值大的样本具有更高的权重。
    2. 子模型权重：如果子模型的效果越好，当前模型的权重越大(当前模型可信度越大)
    思想：通过不断的更改样本的权重，让比较难预测的样本具有更大的权重，这样在模型构建的时候会重点衡量，关注权重大的样本，尽可能让这些样本点预测正确。
    当子模型的个数足够多的时候，所有子模型的融合可以让预测更正确。
    内部：通过不断的基于当前子模型效果更改样本的权重，让预测正确的样本权重减小，预测失败的样本权重增加来解决欠拟合的问题。

十三. 梯度提升迭代决策树GBDT
      GBDT不论是分类还是回归，底层子模型必须是回归树CART模型。
      思想：对于子模型而言，希望子模型的预测值可以让整个模型的预测值更加靠近实际值，也就是实际值和预测值之间的误差最小化，使用损失函数在上一个模型的预测结果基础上的
      反向梯度作为当前子模型的y值(误差)来进行训练。
      内部：GBDT通过更改训练集的Y值，让整个模型的预测结果越来越准。

      算法原理：
      1. 给定训练样本{(X1,Y1),(X2,Y2)...(Xn,Yn)}，找出近似函数F(X)，使得损失函数J(Y,F(X))的损失值最小。
      2. 损失函数一般采用最小二乘或者是绝对值损失函数：
          J(Y,F(X)) = 1/2(Y - F(X))**2  J(Y,F(X)) = |Y - F(X)|
      3. 假设F(x)是由fi(x)的加权和：
          F(x) =v sum(fi(x))  (v为学习步长，缩放系数，防止每个学习器能力过强，导致过拟合)
      4. 由贪心算法的思想可得Fm(x)，求解最优f：
          Fm(x) = Fm-1(x) + fm(X)   arg min(sum J(Y,Fm-1(Xi) +fm(X)))
      5. 给定常数函数F0(X),都可以，一般取均值。
      6. 计算损失函数负梯度值，其实就是y-Fm(X)的差值 ，用差值更改训练数据重新训练得到一个新的模型
      7. 所有树的结果相加就是最终的结果

      GBDT由三部分组成：DT、GB、Shrinkage(衰减)，由多个决策树组成，所有树的结果累加起来就是最终结果。
      GBDT与RF的区别：
      1. RF的每个子模型是没有关系的，而GBDT的每个子模型都是用前一个子模型预测结果与实际结果的残差预测产生的
      2. GBDT构建子树的时候，使用之前子树构建结果后形成的残差作为输入数据构建下一棵子树，然后按照最终预测加过相加得到结果

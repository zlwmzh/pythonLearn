一. 什么是集成学习(Ensemble Learning)？
    思想：将若干学习器(分类器/回归器)合并到一起产生一个新的学习器。
    简单直白理解：就是将多个子模型合并到一起进行模型的预测。常见的有Bagging、Boosting、Stacking。

Note：弱分类器：准确率稍微好于0.5的分类器。

二. 为什么需要Ensemble Learning?
    1. 弱分类器存在一定差异性，这会导致分类的边界不同，也就是说可能存在错误。将多个弱分类器合并后，也就能够得到一个合理的边界，减少整体的
    错误率，实现更好的效果。
    2. 对于数据集过大或过小，可以进行划分和又放回的操作产生不同的数据子集，使用不同的数据子集产生不同的分类器，最终合成一个大的分类器
    3. 如果数据的划分边界过于复杂，使用线性模型很难描述情况，可以训练多个模型，然后进行模型的融合
    4. 对于多个异构的特征集的时候，很难进行融合，那么可以考虑每个数据集构成一个分类模型，然后多个模型融合。（如手机和笔记本）

因为单个模型可能存在偏向，也就导致每个模型有其自由的优点、缺点。“尺有所短寸有所长”
有放回的抽样：
  从数据集中随机抽取一个样本，记录样本信息后，将样本放回到数据集中，然后重复这个抽取操作，这个过程叫做又放回的抽样。
不放回抽样：
  从数据集随机抽取一个样本，记录信息后，从剩下的样本中继续抽取，这个叫做不放回的数据抽样。

三. Bagging
    定义：在具有N个数据样本的原始数据集上，使用有放回的随机抽样抽取N条数据组成的新的样本数据子集，然后用这个数据子集训练子模型。采用这种方式
    训练s个子模型，将这s个子模型合并就是一个Bagging模型。
    目的：解决模型过拟合的问题。
    期望通过多模型的融合，让模型的泛化能力更强，模型更稳定
    Bagging训练的模型在预测分类/回归的时候，使用多数表决或者求平均值的方式来处理。
    Bagging模型底层的子模型只能是同一种类型。

Note：可能存在某些噪声数据仅在部分数据子集中存在，而其他数据子集不存在，基于这样的数据子集训练出来的模型，也就将异常样本造成的影响降低到最低。
Note：因为在数据产生时用的是有放回的数据抽样，所以对于每个数据子集/子模型是存在部分数据没有参与到训练模型中来的，我们将这部分数据称为袋外数据(out of bag)；
当前子模型训练好后，将对应的袋外数据进行评估(score API)，就可以得到当前模型的袋外评估，将所有评估值均值即可得到oob_score;
Note: Bagging方式是有放回的抽样，并且每个数据子集的样本数量必须与原始样本数量一致，模型训练的时候允许存在重复的数据。
Note：差不多存在1/3样本数据是不在Bagging模型的每个子模型的训练数据中的。

训练过程：
    a. 划分得到原始的训练集(m个样本)
    b. 通过有放回的方式获取s个具有m个样本的数据子集
    c. 分布用b中获取的s个样本训练得到s个子模型
    d. 将s个子模型合并得到最终的模型

预测过程：
    a. 带预测样本
    b. 分布用s个子模型对样本进行预测，得到s个预测结果
    c. 如果是分类问题，就通过多数表决计算最终样本分类，如果是回归问题，就通过均值的方式计算样本的目标值。

四. 随机森林(Random Forest)
    在Bagging的基础上进行修改的一种算法。底层的子模型采用的是决策树。
    RF算法过程：
    1. 从原始样本集(m个样本)采用Bootstrap(有放回的抽样)选出m个样本组成的数据集
    2. 使用m个样本组成数据子集训练决策树；从所有的特征属性中随机选取k个特征属性，然后在这k个特征属性中选择最优的划分方式当作当前的
    划分属性，按照这种方式进行迭代生成决策树
    3. 重复上述两步s次，生成s棵决策树
    4. 这s棵决策树形成RF，通过投票表决决定数据分类。

    Note：
    RF算法中存在两个随机：
    1. 对于每个子模型/决策树，用于训练子模型的训练数据是从原始数据中有放回的随机抽样产生的
    2. 在每个决策树构建过程中，对于每个分割点的分割属性的选择都是从所有的特征属性中随机选择k个特征属性，然后再从这k个特征属性中选择最优的作为
    当前节点的分割属性。
    从样本和特征属性两个方面解决过拟合问题。


五. Bagging和RF的联系和区别
    1. 联系：
       RF属于Bagging体系，主要在Bagging上做了一个修改，目的也是缓解过拟合
    2. 区别：
       a. Bagging底层的子模型可以是任意模型(eg:KNN,决策树...)，而RF的底层子模型必须是决策树
       b. RF构建决策树的过程中，特征属性存在随机选择的过程

六. 决策树
    决策树构建的时候，会选择让数据划分最‘纯’的方向进行划分，也就是训练数据上区分能力最强的特征属性以及划分节点进行数据划分(训练数据上最优的划分方式)。在深度
    比较深的时候都是非常契合训练数据。也就可能存在决策树中某个决策路径是契合训练数据，但是对于测试数据是不太友好的。
    Note：在决策树中，一个叶子节点中所有的样本应该是满足相同的数据特征信息的，所以可以认为如果多个样本落在同一个叶子节点中，可以认为这些样本属于同一个类别。

七. Extra Tree: 极端随机森林
    是RF的一个变化，原理基本和RF一样。主要区别：
    1. RF的子模型训练数据都是有放回抽样产生的，也就是数据子集其实是不一样的，而Extra Tree每个子模型的训练数据是完全一样的，直接采用原始数据进行训练
    2. RF生成决策树时会选择最优的划分属性进行分割，而Extra Tree则是每次直接从特征属性中随机一个特征，并且随机选择一个分割点（value）进行数据划分。
    Note：一般情况下Extra Tree 决策树规模大于RF。方差相对于RF进一步减少

八. Totally Random Trees Embedding(TRTE)
    利用决策树中，多个样本落在相同的叶子节点的话，可以认为属于同一个类别的，那么构建多颗决策树，然后将样本落在叶子节点的信息转化为特征矩阵的形式，从而达到一个
    维度扩展的效果。(类似多项式扩展)
    和RF的区别：
    1. 随机森林的子模型训练数据是有放回抽样产生，那也就是每个子数据集是不一样的，TRTE每个子模型的训练数据是完全一样的
    2. RF是有监督学习模型，TRTE是无监督的学习模型
    3. TRTE底层实际上是回归树(每个子模型的Y值是完全随机的)
	
九. Isolation Forest(IForest)	
	IForest是一种异常点检测算法，使用类似RF的方式检测异常点;
	和RF的区别：
	1. RF的子模型的训练数据都是有放回的随机抽样产生的，每个子模型的训练数据集都是不同的。IForest的每个子模型的训练数据都是基于原始数据采取不放回的抽样
	产生的部分数据进行模型训练（正常样本数据，默认最少256条数据参与模型训练，不满256条的话全部参与训练）
	2. IForest 和TRTF 是完全一样的，每个子模型的Y值是完全随机的，因为子模型仅仅是为了学习数据特征
	3. IForest认为越靠近根节点的叶子节点越可能是异常样本点	
	Note：学习正常样本的所有特征，所有不满足正常样本特征的都属于异常样本	
	
十. RF总结：
  ·1. 主要优点:
      a. 训练可以并行化，对于大规模训练具有速度优势
	  b. 优化随机选择特征属性进行分割划分，所以对于高纬度数据，仍然具有比较高的训练性能。 	
	  c. 可以给出各个特征属性的重要性列表
	  e. 由于存在随机抽样，泛化能力强，方差小，能够缓解过拟合情况
	  f. 实现简单
	  g: 对于缺失的特征属性不敏感
	2. 主要缺点：
      a. 在某些噪音比较大的特征上（数据特别异常情况），RF模型容易陷入过拟合； 
      b. 取值比较多的划分特征对RF的决策会产生更大的影响，从而有可能影响模型的效果。
	  
	  

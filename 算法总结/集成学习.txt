集成学习
=========================================================
集成学习	
	简单直白理解：就是将多个子模型（学习器[分类器&回归器]）聚合到一起进行模型的预测。常见：Bagging、Boosting、Stacking
	假设有如下三个数据集：
	数据集A：类别、金额、点击次数、下单时间...
	手机，5000，5，2019-01-01，...
	日用品，100，1，2019-05-01，...
	数据集B（手机数据）：具体类别、分辨率、待机时长，CPU...
	小米5A，5，12，...
	小米6A，6，24，...
	数据集C（日用品数据）：...
	数据集D（电脑数据）：CPU、分辨率.....
	
	因为单个模型可能存在偏向，也就会导致每个模型有其自有的优点、缺点。"尺有所短寸有所长"
	-1. Bagging
		期望通过多模型的融合，让模型的泛化能力更强，模型更加稳定。	 --> 可以解决过拟合  所有的模型必须是一样的
		因为每个子模型可能存在各自的缺陷（偏向），也就说每个模型可能对于某些样本的取值会有比较合适的越策，对于另外某些样本
		取值可能存在过拟合的情况，这个时候通过多个模型的合并，可以缓解这个问题。
	-2. Boosting
		期望通过多模型的融合，让模型的效果预测能力更强，对于训练数据的预测效果更好。 --> 可以解决欠拟合
	-3. Stacking
		期望通过不同算法模型的融合，让整个模型更加稳定，效果更加的好。可能效果很好，可能效果很差。
		
		
有放回的抽样：
	从数据集中随机抽取一个样本，记录样本的信息后，将样本放回数据集中，然后重复这个抽取的操作，这个过程就叫做有放回的抽样
不放回的抽样：
    从数据集中随机抽取一个样本，记录样本的信息，然后从剩下的样本数据集中在抽取下一条样本，并记录信息，这个过程就叫做不放回的抽样
Bagging:
    主要目的：缓解过拟合（噪声样本存在导致过拟合）
    在具有N条数据的原始数据集中，使用由放回的抽样，抽取出N条数据组成一个数据子集，然后使用数据自己构建一个子模型，采用这样的方式分布训练s个子模型
	，将这s个子模型合并就是Bagging的模型。
	Note：可能存在某些噪声样本仅在部分数据子集中存在，而在其他数据子集中不存在，基于这样的数据子集训练出来的模型，也就是会将异常样本对于模型的影响降低到最低。
	Note：因为在产生数据的时候使用的是有放回的抽样，所以对于每个数据子集/每个子模型是存在部分数据没有参与进来的情况，这部分没有包含进来的数据就叫做袋外数据（out
	 of bag）,将当前子模型训练好后，将对应的袋外数据进行评估（score API），就可以得到当前模型上的袋外的评估值，将所有的评估值均值即可得到oob_score